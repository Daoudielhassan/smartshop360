"""
src/agent/graph.py
===================
Cha√Æne LLM (Text-to-SQL) ‚Äî compatible multi-provider.

Providers support√©s (par ordre de priorit√©) :
  1. Groq      GROQ_API_KEY      ‚Üí llama-3.3-70b-versatile
  2. Mistral   MISTRAL_API_KEY   ‚Üí mistral-large-latest
  3. OpenAI    OPENAI_API_KEY    ‚Üí gpt-4o-mini
  4. Anthropic ANTHROPIC_API_KEY ‚Üí claude-3-5-haiku-20241022
  5. Fallback  ‚Äî                 ‚Üí r√®gles SQL bas√©es sur mots-cl√©s

Point d'entr√©e public :
    run_agent(question, api_key, conversation_history) ‚Üí dict
    get_active_provider(api_key) ‚Üí str
"""

import os
import json
import re
import sys
import requests
from dotenv import load_dotenv

load_dotenv()
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))

from src.agent.tools import execute_sql, python_analysis, SYSTEM_PROMPT


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  D√©tection du provider
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _detect_provider(api_key: str | None = None) -> tuple:
    """Auto-d√©tecte le provider LLM disponible. Retourne (name, key)."""
    if api_key:
        if api_key.startswith("gsk_"):                              return ("groq",      api_key)
        if api_key.startswith("sk-ant-"):                           return ("anthropic", api_key)
        if api_key.startswith("sk-"):                               return ("openai",    api_key)
        if 28 <= len(api_key) <= 64 and not api_key.startswith("sk"):
            return ("mistral", api_key)

    for env, name in [
        ("GROQ_API_KEY",      "groq"),
        ("MISTRAL_API_KEY",   "mistral"),
        ("OPENAI_API_KEY",    "openai"),
        ("ANTHROPIC_API_KEY", "anthropic"),
    ]:
        val = os.environ.get(env)
        if val:
            return (name, val)

    return ("fallback", "")


def get_active_provider(api_key: str | None = None) -> str:
    """Retourne une cha√Æne lisible d√©crivant le provider actif."""
    labels = {
        "groq":      "üü¢ Groq (Llama 3.3-70B)",
        "mistral":   "üîµ Mistral (Large)",
        "openai":    "üü† OpenAI (GPT-4o-mini)",
        "anthropic": "üü£ Anthropic (Claude 3.5 Haiku)",
        "fallback":  "‚ö´ Mode Hors-ligne (r√®gles SQL)",
    }
    provider, _ = _detect_provider(api_key)
    return labels.get(provider, "‚ùì Inconnu")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  Appels individuels aux providers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _call_groq(messages: list, system: str, key: str, max_tokens: int = 1024) -> str:
    r = requests.post(
        "https://api.groq.com/openai/v1/chat/completions",
        headers={"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        json={
            "model":      "llama-3.3-70b-versatile",
            "max_tokens": max_tokens,
            "messages":   [{"role": "system", "content": system}] + messages,
        },
        timeout=30,
    )
    if r.status_code == 200:
        return r.json()["choices"][0]["message"]["content"]
    raise RuntimeError(f"Groq {r.status_code}: {r.text[:200]}")


def _call_mistral(messages: list, system: str, key: str, max_tokens: int = 1024) -> str:
    r = requests.post(
        "https://api.mistral.ai/v1/chat/completions",
        headers={"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        json={
            "model":      "mistral-large-latest",
            "max_tokens": max_tokens,
            "messages":   [{"role": "system", "content": system}] + messages,
        },
        timeout=30,
    )
    if r.status_code == 200:
        return r.json()["choices"][0]["message"]["content"]
    raise RuntimeError(f"Mistral {r.status_code}: {r.text[:200]}")


def _call_openai(messages: list, system: str, key: str, max_tokens: int = 1024) -> str:
    r = requests.post(
        "https://api.openai.com/v1/chat/completions",
        headers={"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        json={
            "model":      "gpt-4o-mini",
            "max_tokens": max_tokens,
            "messages":   [{"role": "system", "content": system}] + messages,
        },
        timeout=30,
    )
    if r.status_code == 200:
        return r.json()["choices"][0]["message"]["content"]
    raise RuntimeError(f"OpenAI {r.status_code}: {r.text[:200]}")


def _call_anthropic(messages: list, system: str, key: str, max_tokens: int = 1024) -> str:
    r = requests.post(
        "https://api.anthropic.com/v1/messages",
        headers={
            "x-api-key":         key,
            "anthropic-version": "2023-06-01",
            "Content-Type":      "application/json",
        },
        json={
            "model":      "claude-3-5-haiku-20241022",
            "max_tokens": max_tokens,
            "system":     system,
            "messages":   messages,
        },
        timeout=30,
    )
    if r.status_code == 200:
        return r.json()["content"][0]["text"]
    raise RuntimeError(f"Anthropic {r.status_code}: {r.text[:200]}")


def call_llm(messages: list, api_key: str | None = None, max_tokens: int = 1024) -> str:
    """Route vers le bon provider et g√®re le fallback."""
    provider, key = _detect_provider(api_key)
    callers = {
        "groq":      _call_groq,
        "mistral":   _call_mistral,
        "openai":    _call_openai,
        "anthropic": _call_anthropic,
    }
    if provider in callers:
        try:
            return callers[provider](messages, SYSTEM_PROMPT, key, max_tokens)
        except Exception as e:
            print(f"[graph] Provider {provider} √©chou√© : {e} ‚Äî basculement fallback")

    return _fallback_sql(messages[-1]["content"] if messages else "")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  Fallback : r√®gles SQL par mots-cl√©s
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _fallback_sql(question: str) -> str:
    q = question.lower()
    if any(w in q for w in ["critique", "alerte", "mauvais", "n√©gatif"]):
        sql = 'SELECT "ProductName", "Notemoyenne", "AvisNegatifs", "Statut" FROM v_alerts WHERE "Statut" IN (\'CRITIQUE\', \'A_SURVEILLER\') ORDER BY "Notemoyenne" ASC LIMIT 10'
    elif any(w in q for w in ["produit", "vente", "best", "top", "meilleur"]):
        sql = 'SELECT "ProductName", "Category", "CA", "QuantiteVendue", "Notemoyenne" FROM v_product_kpi ORDER BY "CA" DESC LIMIT 10'
    elif any(w in q for w in ["client", "acheteur", "fid√®le"]):
        sql = 'SELECT "Nom", "Pays", "NbCommandes", "CA_Total" FROM v_customer_kpi ORDER BY "CA_Total" DESC LIMIT 10'
    elif any(w in q for w in ["avis", "note", "sentiment", "satisfaction"]):
        sql = 'SELECT "ProductName", "Notemoyenne", "NbAvis", "AvisPositifs", "AvisNegatifs" FROM v_product_kpi ORDER BY "Notemoyenne" DESC LIMIT 10'
    elif any(w in q for w in ["qualit√©", "mdm", "couverture", "mapping"]):
        sql = 'SELECT * FROM v_data_quality'
    elif any(w in q for w in ["ca", "chiffre", "revenu", "revenue"]):
        sql = 'SELECT "Category", SUM("CA") AS "CA_Categorie" FROM v_product_kpi GROUP BY "Category" ORDER BY "CA_Categorie" DESC'
    else:
        sql = 'SELECT "ProductName", "CA", "Notemoyenne" FROM v_product_kpi ORDER BY "CA" DESC LIMIT 10'

    return json.dumps({
        "sql":             sql,
        "reasoning":       "Mode hors-ligne ‚Äî r√®gle SQL par mots-cl√©s.",
        "answer_template": "Voici les r√©sultats bas√©s sur votre question.",
    })


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  Formatage de la r√©ponse naturelle
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def format_natural_response(question: str, sql: str, data: list,
                             api_key: str | None = None) -> str:
    if not data:
        return "Aucun r√©sultat trouv√© pour cette requ√™te."

    preview = str(data[:5])
    messages = [{
        "role": "user",
        "content": (
            f"Question : {question}\n"
            f"SQL ex√©cut√© : {sql}\n"
            f"Donn√©es (extrait) : {preview}\n"
            f"Nombre total de lignes : {len(data)}\n\n"
            "Formule une r√©ponse claire et synth√©tique en fran√ßais pour un manager."
        ),
    }]

    provider, key = _detect_provider(api_key)
    callers = {
        "groq":      _call_groq,
        "mistral":   _call_mistral,
        "openai":    _call_openai,
        "anthropic": _call_anthropic,
    }
    if provider in callers:
        try:
            return callers[provider](messages, "Tu es un assistant data analytique concis.", key, 512)
        except Exception:
            pass

    # Fallback textuel
    lines = [f"**{k}** : {v}" for k, v in data[0].items()]
    return (
        f"J'ai trouv√© **{len(data)} r√©sultat(s)**."
        f"\n\nPremier r√©sultat :\n" + "\n".join(lines)
    )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  Point d'entr√©e principal : run_agent
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def run_agent(question: str, api_key: str | None = None,
              conversation_history: list | None = None) -> dict:
    """
    Orchestre le pipeline Text-to-SQL :
      1. LLM g√©n√®re le SQL (ou fallback)
      2. SQL ex√©cut√© sur PostgreSQL
      3. LLM formule la r√©ponse naturelle

    Returns
    -------
    dict avec cl√©s : question, sql, reasoning, data, columns,
                     answer, row_count, success
    """
    history = conversation_history or []

    messages = history + [{"role": "user", "content": question}]

    # √âtape 1 : g√©n√©ration SQL
    llm_raw = call_llm(messages, api_key)

    # Extraction JSON (le LLM peut inclure du texte autour)
    sql, reasoning, answer_template = "", "", ""
    try:
        match = re.search(r"\{.*\}", llm_raw, re.DOTALL)
        if match:
            parsed       = json.loads(match.group())
            sql          = parsed.get("sql", "")
            reasoning    = parsed.get("reasoning", "")
            answer_template = parsed.get("answer_template", "")
    except (json.JSONDecodeError, AttributeError):
        sql = llm_raw  # dernier recours : traiter la r√©ponse brute comme SQL

    # √âtape 2 : ex√©cution SQL
    sql_result = execute_sql(sql) if sql else {"success": False, "error": "SQL vide", "data": [], "columns": [], "row_count": 0}

    # √âtape 3 : r√©ponse naturelle
    if sql_result["success"] and sql_result["data"]:
        answer = format_natural_response(question, sql, sql_result["data"], api_key)
    elif sql_result["success"]:
        answer = "La requ√™te n'a retourn√© aucun r√©sultat."
    else:
        answer = f"Erreur SQL : {sql_result.get('error', 'inconnue')}"

    return {
        "question":  question,
        "sql":       sql,
        "reasoning": reasoning,
        "data":      sql_result.get("data", []),
        "columns":   sql_result.get("columns", []),
        "answer":    answer,
        "row_count": sql_result.get("row_count", 0),
        "success":   sql_result.get("success", False),
    }
